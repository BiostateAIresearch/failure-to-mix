{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failure to Mix - Complete Experiment Runner\n",
    "\n",
    "This notebook runs all experiments and generates all figures for the paper:\n",
    "- Figure 1: Single flip baseline (8 models)\n",
    "- Figure 2: Multiple decisions D=2, D=3\n",
    "- Figure 3: D=10 ensemble analysis\n",
    "- Figure 4: Ternary distribution (0/1/2)\n",
    "- Figure 5: Word bias experiments\n",
    "- Figure 6: Game theory applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip -q install aiohttp pandas matplotlib google-api-python-client tqdm nest_asyncio"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Google Drive integration (optional)\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    from googleapiclient.discovery import build\n",
    "    from googleapiclient.http import MediaFileUpload\n",
    "    auth.authenticate_user()\n",
    "    drive = build('drive', 'v3')\n",
    "    sheets = build('sheets', 'v4')\n",
    "    DRIVE_ENABLED = True\n",
    "except:\n",
    "    DRIVE_ENABLED = False\n",
    "    print(\"Google Drive not available, saving locally only\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# API Configuration\n",
    "API_KEYS = [\n",
    "    \"sk-or-v1-YOUR-KEY-1\",  # Replace with your OpenRouter API keys\n",
    "    \"sk-or-v1-YOUR-KEY-2\",\n",
    "]\n",
    "API_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "# Experiment settings\n",
    "BATCH_SIZE = 20\n",
    "INITIAL_WAIT = 2.0\n",
    "MAX_RETRIES = 3\n",
    "SAVE_PATH = \"/content/results\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Model colors (matching paper)\n",
    "MODEL_COLORS = {\n",
    "    \"google/gemini-2.5-pro\": \"#1f77b4\",       # Blue\n",
    "    \"openai/gpt-5\": \"#2ca02c\",                 # Green\n",
    "    \"openai/gpt-5-nano\": \"#d62a28\",            # Red\n",
    "    \"anthropic/claude-4.5-sonnet\": \"#8c564b\",  # Brown-purple\n",
    "    \"moonshotai/kimi-k2-0905\": \"#e377c2\",      # Pink\n",
    "    \"qwen/qwen3-vl-8b-instruct\": \"#662d91\",    # Purple\n",
    "    \"x-ai/grok-4-fast\": \"#c2b59b\",             # Tan\n",
    "    \"deepseek/deepseek-v3.2\": \"#9467bd\",       # Purple\n",
    "}\n",
    "\n",
    "EDGE_COLOR = \"#999b9e\"\n",
    "\n",
    "def get_color(model):\n",
    "    model_lower = model.lower()\n",
    "    for key, color in MODEL_COLORS.items():\n",
    "        if key.lower() in model_lower or model_lower in key.lower():\n",
    "            return color\n",
    "    # Provider fallback\n",
    "    if \"gemini\" in model_lower: return \"#1f77b4\"\n",
    "    if \"gpt\" in model_lower:\n",
    "        if \"nano\" in model_lower: return \"#d62a28\"\n",
    "        return \"#2ca02c\"\n",
    "    if \"claude\" in model_lower or \"sonnet\" in model_lower: return \"#8c564b\"\n",
    "    if \"kimi\" in model_lower: return \"#e377c2\"\n",
    "    if \"qwen\" in model_lower: return \"#662d91\"\n",
    "    if \"grok\" in model_lower: return \"#c2b59b\"\n",
    "    if \"deepseek\" in model_lower: return \"#9467bd\"\n",
    "    return \"#7f7f7f\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Response parsing\n",
    "def parse_response(d: dict):\n",
    "    \"\"\"Parse API response to extract content and reasoning.\"\"\"\n",
    "    msg = (d.get(\"choices\") or [{}])[0].get(\"message\", {})\n",
    "    content = msg.get(\"content\") or \"\"\n",
    "    reasoning = msg.get(\"reasoning\") or \"\"\n",
    "    if not reasoning:\n",
    "        rd = msg.get(\"reasoning_details\")\n",
    "        if isinstance(rd, list) and rd:\n",
    "            reasoning = rd[0].get(\"summary\") or rd[0].get(\"data\") or \"\"\n",
    "    merged = (content or reasoning or \"\").strip()\n",
    "    return content.strip(), reasoning.strip(), merged\n",
    "\n",
    "def extract_binary(text: str) -> str:\n",
    "    \"\"\"Extract 0/1 from response.\"\"\"\n",
    "    if not text: return \"error\"\n",
    "    t = text.strip()\n",
    "    if t == \"0\": return \"0\"\n",
    "    if t == \"1\": return \"1\"\n",
    "    m = re.findall(r\"\\b([01])\\b\", t)\n",
    "    if m: return m[-1]\n",
    "    if \"1\" in t and \"0\" not in t: return \"1\"\n",
    "    if \"0\" in t and \"1\" not in t: return \"0\"\n",
    "    low = t.lower()\n",
    "    if \"one\" in low and \"zero\" not in low: return \"1\"\n",
    "    if \"zero\" in low and \"one\" not in low: return \"0\"\n",
    "    return \"error\"\n",
    "\n",
    "def extract_heads_tails(text: str) -> str:\n",
    "    \"\"\"Extract Heads/Tails from response.\"\"\"\n",
    "    if not text: return \"error\"\n",
    "    m = re.findall(r\"(Heads|Tails)\", text, re.IGNORECASE)\n",
    "    if m: return m[-1].capitalize()\n",
    "    return \"error\"\n",
    "\n",
    "def extract_ternary(text: str) -> str:\n",
    "    \"\"\"Extract 0/1/2 from response.\"\"\"\n",
    "    if not text: return \"error\"\n",
    "    t = text.strip()\n",
    "    if t in [\"0\", \"1\", \"2\"]: return t\n",
    "    m = re.findall(r\"\\b([012])\\b\", t)\n",
    "    if m: return m[-1]\n",
    "    return \"error\"\n",
    "\n",
    "def extract_word(text: str, word1: str, word2: str) -> str:\n",
    "    \"\"\"Extract word choice from response.\"\"\"\n",
    "    if not text: return \"error\"\n",
    "    t = text.strip().lower()\n",
    "    w1, w2 = word1.lower(), word2.lower()\n",
    "    if t == w1: return word1\n",
    "    if t == w2: return word2\n",
    "    pattern = f\"({re.escape(w1)}|{re.escape(w2)})\"\n",
    "    m = re.findall(pattern, t, re.IGNORECASE)\n",
    "    if m:\n",
    "        return word1 if m[-1].lower() == w1 else word2\n",
    "    return \"error\"\n",
    "\n",
    "# S metric calculation\n",
    "def compute_S(p_vals, r_vals, extend=True):\n",
    "    \"\"\"Compute step-likeness metric S.\"\"\"\n",
    "    p = np.asarray(p_vals, dtype=float)\n",
    "    r = np.asarray(r_vals, dtype=float)\n",
    "    mask = np.isfinite(p) & np.isfinite(r)\n",
    "    p, r = p[mask], r[mask]\n",
    "    if len(p) < 2: return np.nan\n",
    "    order = np.argsort(p)\n",
    "    p, r = p[order], r[order]\n",
    "    # Convert to [0,1] if percentage\n",
    "    if p.max() > 1:\n",
    "        p = p / 100.0\n",
    "        r = r / 100.0\n",
    "    # Extend to boundaries\n",
    "    if extend:\n",
    "        if p[0] > 0:\n",
    "            p = np.insert(p, 0, 0)\n",
    "            r = np.insert(r, 0, r[0])\n",
    "        if p[-1] < 1:\n",
    "            p = np.append(p, 1)\n",
    "            r = np.append(r, r[-1])\n",
    "    # Trapezoid integration\n",
    "    dp = np.diff(p)\n",
    "    left = np.abs(r[:-1] - p[:-1])\n",
    "    right = np.abs(r[1:] - p[1:])\n",
    "    trap = np.sum(dp * (left + right) / 2.0)\n",
    "    return 4.0 * trap"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# API caller\n",
    "async def call_model(session, model, messages, api_key, max_tokens=2, temperature=None, attempt=1):\n",
    "    \"\"\"Make API call with retry logic.\"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\"model\": model, \"messages\": messages}\n",
    "    if max_tokens: payload[\"max_tokens\"] = max_tokens\n",
    "    if temperature is not None: payload[\"temperature\"] = temperature\n",
    "    \n",
    "    try:\n",
    "        async with session.post(API_URL, json=payload, headers=headers, timeout=120) as r:\n",
    "            if r.status == 429:\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    await asyncio.sleep(3 * attempt)\n",
    "                    return await call_model(session, model, messages, api_key, max_tokens, temperature, attempt+1)\n",
    "                return {\"status\": \"rate_limited\", \"content\": \"\"}\n",
    "            d = await r.json()\n",
    "            content, reasoning, merged = parse_response(d)\n",
    "            return {\"status\": \"success\", \"content\": content, \"reasoning\": reasoning, \"raw\": d}\n",
    "    except Exception as e:\n",
    "        if attempt < MAX_RETRIES:\n",
    "            await asyncio.sleep(2 * attempt)\n",
    "            return await call_model(session, model, messages, api_key, max_tokens, temperature, attempt+1)\n",
    "        return {\"status\": str(e), \"content\": \"\"}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1: Single Flip Baseline\n",
    "\n",
    "Tests 8 models on simple binary coin flip with p from 0-100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Figure 1 - Single flip experiment\n",
    "MODELS_FIG1 = [\n",
    "    \"google/gemini-2.5-pro\",\n",
    "    \"openai/gpt-5\",\n",
    "    \"openai/gpt-5-nano\",\n",
    "    \"anthropic/claude-4.5-sonnet\",\n",
    "    \"moonshotai/kimi-k2-0905\",\n",
    "    \"qwen/qwen3-vl-8b-instruct\",\n",
    "    \"x-ai/grok-4-fast\",\n",
    "    \"deepseek/deepseek-v3.2\",\n",
    "]\n",
    "\n",
    "PROMPT_SINGLE_FLIP = 'I\\'d like you to flip a biased coin that returns \"1\" {p}% of the time and \"0\" {100_p}% of the time. Please return a 1 token response simulating a single flip.'\n",
    "\n",
    "async def run_fig1_experiment(model, p_values, n_trials=100):\n",
    "    \"\"\"Run single flip experiment for one model.\"\"\"\n",
    "    results = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for p in tqdm(p_values, desc=f\"{model.split('/')[-1]}\"):\n",
    "            prompt = PROMPT_SINGLE_FLIP.format(p=p, **{\"100_p\": 100-p})\n",
    "            for trial in range(n_trials):\n",
    "                api_key = API_KEYS[(p * n_trials + trial) % len(API_KEYS)]\n",
    "                resp = await call_model(\n",
    "                    session, model,\n",
    "                    [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    api_key\n",
    "                )\n",
    "                answer = extract_binary(resp[\"content\"])\n",
    "                results.append({\"p\": p, \"trial\": trial, \"answer\": answer, \"status\": resp[\"status\"]})\n",
    "                await asyncio.sleep(INITIAL_WAIT / BATCH_SIZE)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def analyze_fig1(df):\n",
    "    \"\"\"Compute r for each p value.\"\"\"\n",
    "    valid = df[df[\"answer\"].isin([\"0\", \"1\"])]\n",
    "    summary = valid.groupby(\"p\").apply(lambda x: (x[\"answer\"]==\"1\").mean() * 100).reset_index()\n",
    "    summary.columns = [\"p\", \"r\"]\n",
    "    return summary"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot Figure 1c - Full Gemini curve\n",
    "def plot_fig1c(summary, model_name, save_path=None):\n",
    "    \"\"\"Plot full p-r curve (0-100).\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    p, r = summary[\"p\"].values, summary[\"r\"].values\n",
    "    color = get_color(model_name)\n",
    "    \n",
    "    ax.plot(p, r, color=color, linewidth=1.8, zorder=2)\n",
    "    ax.scatter(p, r, s=50, facecolors=color, edgecolors='black', linewidths=0.6, zorder=3)\n",
    "    ax.plot([0, 100], [0, 100], '--', color='gray', alpha=0.5, linewidth=0.8)\n",
    "    ax.axvline(x=50, color=EDGE_COLOR, linewidth=0.9, alpha=0.3)\n",
    "    \n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_xlabel(\"p (%)\", fontweight='bold')\n",
    "    ax.set_ylabel(\"r (%)\", fontweight='bold')\n",
    "    ax.set_title(f\"{model_name}\", fontweight='bold')\n",
    "    \n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color(EDGE_COLOR)\n",
    "    ax.tick_params(color=EDGE_COLOR, labelcolor=EDGE_COLOR)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', transparent=True)\n",
    "        plt.savefig(save_path.replace('.png', '.svg'), format='svg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Plot Figure 1e - Zoomed comparison (45-55)\n",
    "def plot_fig1e(all_results, save_path=None):\n",
    "    \"\"\"Plot 8 models zoomed to p=45-55.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (model, summary) in enumerate(all_results.items()):\n",
    "        ax = axes[idx]\n",
    "        mask = (summary[\"p\"] >= 45) & (summary[\"p\"] <= 55)\n",
    "        sub = summary[mask]\n",
    "        color = get_color(model)\n",
    "        \n",
    "        ax.plot(sub[\"p\"], sub[\"r\"], 'o-', color=color, markersize=8,\n",
    "                markeredgecolor='black', markeredgewidth=0.6)\n",
    "        ax.axvline(x=50, color=EDGE_COLOR, linewidth=0.9, alpha=0.3)\n",
    "        \n",
    "        ax.set_xlim(45, 55)\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_title(model.split('/')[-1], fontsize=11, color=color)\n",
    "        ax.set_xlabel(\"p (%)\")\n",
    "        ax.set_ylabel(\"r (%)\")\n",
    "        \n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_color(EDGE_COLOR)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return fig"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2: Multiple Decisions (D=2, D=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Figure 2 - Multiple decisions\n",
    "PROMPT_MULTI_FLIP = 'I\\'d like you to flip a biased coin that returns \"1\" {p}% of the time and \"0\" {100_p}% of the time. Please return a {D} token response simulating {D} flips. Please return exactly {D} tokens (0 or 1) separated by a single space.'\n",
    "\n",
    "async def run_fig2_experiment(model, p_values, D=2, n_trials=100):\n",
    "    \"\"\"Run multi-flip experiment.\"\"\"\n",
    "    results = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for p in tqdm(p_values, desc=f\"D={D}\"):\n",
    "            prompt = PROMPT_MULTI_FLIP.format(p=p, D=D, **{\"100_p\": 100-p})\n",
    "            for trial in range(n_trials):\n",
    "                api_key = API_KEYS[(p * n_trials + trial) % len(API_KEYS)]\n",
    "                resp = await call_model(\n",
    "                    session, model,\n",
    "                    [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    api_key,\n",
    "                    max_tokens=D * 2 + 5\n",
    "                )\n",
    "                # Parse D responses\n",
    "                text = resp[\"content\"]\n",
    "                if \" \" in text:\n",
    "                    tokens = text.split()\n",
    "                else:\n",
    "                    tokens = list(text)\n",
    "                tokens = [t for t in tokens if t in [\"0\", \"1\"]]\n",
    "                tokens = (tokens + [None] * D)[:D]  # Pad/truncate\n",
    "                \n",
    "                for j, tok in enumerate(tokens):\n",
    "                    results.append({\n",
    "                        \"p\": p, \"trial\": trial, \"D\": D, \"j\": j+1,\n",
    "                        \"answer\": tok if tok else \"error\",\n",
    "                        \"status\": resp[\"status\"]\n",
    "                    })\n",
    "                await asyncio.sleep(INITIAL_WAIT / BATCH_SIZE)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def analyze_fig2(df, j=None):\n",
    "    \"\"\"Compute r for specific j or mean across all j.\"\"\"\n",
    "    if j is not None:\n",
    "        sub = df[df[\"j\"] == j]\n",
    "    else:\n",
    "        sub = df\n",
    "    valid = sub[sub[\"answer\"].isin([\"0\", \"1\"])]\n",
    "    summary = valid.groupby(\"p\").apply(lambda x: (x[\"answer\"]==\"1\").mean() * 100).reset_index()\n",
    "    summary.columns = [\"p\", \"r\"]\n",
    "    return summary"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot Figure 2b - j=1 response (step function)\n",
    "def plot_fig2b(summary, model_name, D, save_path=None):\n",
    "    \"\"\"Plot first decision response curve.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    p, r = summary[\"p\"].values, summary[\"r\"].values\n",
    "    S = compute_S(p, r)\n",
    "    \n",
    "    ax.plot(p, r, 'o-', color=get_color(model_name), markersize=6)\n",
    "    ax.plot([0, 100], [0, 100], '--', color='gray', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_xlabel(\"p (%)\")\n",
    "    ax.set_ylabel(\"r (%)\")\n",
    "    ax.set_title(f\"{model_name}\\nD = {D}, j = 1 (first output)\\nS = {S:.3f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    return fig, S\n",
    "\n",
    "# Plot Figure 2c - j=2 zigzag\n",
    "def plot_fig2c(all_j2_results, D=2, save_path=None):\n",
    "    \"\"\"Plot second decision zigzag for multiple models.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for model, summary in all_j2_results.items():\n",
    "        ax.plot(summary[\"p\"], summary[\"r\"], 'o-', label=model.split('/')[-1],\n",
    "                color=get_color(model), markersize=5)\n",
    "    \n",
    "    ax.axvline(x=50, color=EDGE_COLOR, linewidth=0.9, alpha=0.3)\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_xlabel(\"p (%)\")\n",
    "    ax.set_ylabel(\"r (%)\")\n",
    "    ax.set_title(f\"D = {D}, j = 2 (second output)\")\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Plot Figure 2e - Mean(r) comparison\n",
    "def plot_fig2e(mean_results_d2, mean_results_d3, save_path=None):\n",
    "    \"\"\"Plot mean response curves for D=2 and D=3.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for ax, (results, D) in zip(axes, [(mean_results_d2, 2), (mean_results_d3, 3)]):\n",
    "        for model, summary in results.items():\n",
    "            S = compute_S(summary[\"p\"].values, summary[\"r\"].values)\n",
    "            ax.plot(summary[\"p\"], summary[\"r\"], 'o-',\n",
    "                    label=f\"{model.split('/')[-1]} (S={S:.3f})\",\n",
    "                    color=get_color(model), markersize=4)\n",
    "        \n",
    "        ax.plot([0, 100], [0, 100], '--', color='gray', alpha=0.5)\n",
    "        ax.set_xlim(0, 100)\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_xlabel(\"p (%)\")\n",
    "        ax.set_ylabel(\"Mean(r) (%)\")\n",
    "        ax.set_title(f\"Mean(r), D = {D}\")\n",
    "        ax.legend(fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    return fig"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3: D=10 Ensemble Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Figure 3 - D=10 ensemble\n",
    "from scipy.stats import binom\n",
    "\n",
    "async def run_fig3_experiment(model, p_values, D=10, n_trials=1000):\n",
    "    \"\"\"Run D=10 experiment with more trials.\"\"\"\n",
    "    # Same as fig2 but with D=10 and N=1000\n",
    "    return await run_fig2_experiment(model, p_values, D=D, n_trials=n_trials)\n",
    "\n",
    "def plot_fig3a(df, model_name, save_path=None):\n",
    "    \"\"\"Plot individual j response curves for D=10.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    for ax, j in zip(axes.flatten(), [1, 2, 9, 10]):\n",
    "        summary = analyze_fig2(df, j=j)\n",
    "        S = compute_S(summary[\"p\"].values, summary[\"r\"].values)\n",
    "        \n",
    "        ax.plot(summary[\"p\"], summary[\"r\"], 'o-', color=get_color(model_name))\n",
    "        ax.plot([0, 100], [0, 100], '--', color='gray', alpha=0.5)\n",
    "        ax.set_xlim(0, 100)\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_title(f\"D = 10, j = {j}\\nS = {S:.3f}\")\n",
    "        ax.set_xlabel(\"p (%)\")\n",
    "        ax.set_ylabel(\"r (%)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def plot_fig3c(all_model_dfs, p_test, D=10, save_path=None):\n",
    "    \"\"\"Plot histogram of sum of 1s for specific p values.\"\"\"\n",
    "    n_models = len(all_model_dfs)\n",
    "    fig, axes = plt.subplots(2, n_models, figsize=(4*n_models, 8))\n",
    "    \n",
    "    p_vals = [15, 45]  # Two p values to test\n",
    "    \n",
    "    for row, p in enumerate(p_vals):\n",
    "        for col, (model, df) in enumerate(all_model_dfs.items()):\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            # Count sum of 1s per trial\n",
    "            trial_sums = df[df[\"p\"] == p].groupby(\"trial\").apply(\n",
    "                lambda x: (x[\"answer\"] == \"1\").sum()\n",
    "            )\n",
    "            \n",
    "            # Expected binomial\n",
    "            x = np.arange(D + 1)\n",
    "            expected = binom.pmf(x, D, p/100) * len(trial_sums)\n",
    "            observed, _ = np.histogram(trial_sums, bins=np.arange(D + 2) - 0.5)\n",
    "            \n",
    "            ax.bar(x - 0.2, expected, width=0.4, label='Expected i.i.d.', alpha=0.7)\n",
    "            ax.bar(x + 0.2, observed, width=0.4, label='Observed', alpha=0.7,\n",
    "                   color=get_color(model))\n",
    "            \n",
    "            ax.set_xlabel(\"Number of '1' outcomes\")\n",
    "            ax.set_ylabel(\"Frequency\")\n",
    "            ax.set_title(f\"{model.split('/')[-1]}\\np = {p/100}\")\n",
    "            if row == 0 and col == 0:\n",
    "                ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    return fig"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 4: Ternary Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Figure 4 - Ternary distribution (0/1/2)\n",
    "PROMPT_TERNARY = 'I\\'d like to draw from a biased deck that returns \"2\" {q}% of the time, \"1\" {p}% of the time and \"0\" {r}% of the time. Please return a 1 token response simulating a single draw.'\n",
    "\n",
    "async def run_fig4_experiment(model, q_values, p_fixed=40, n_trials=100):\n",
    "    \"\"\"Run ternary distribution experiment.\"\"\"\n",
    "    results = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for q in tqdm(q_values, desc=f\"Ternary\"):\n",
    "            r_val = 100 - p_fixed - q\n",
    "            if r_val < 0:\n",
    "                continue\n",
    "            prompt = PROMPT_TERNARY.format(q=q, p=p_fixed, r=r_val)\n",
    "            \n",
    "            for trial in range(n_trials):\n",
    "                api_key = API_KEYS[(q * n_trials + trial) % len(API_KEYS)]\n",
    "                resp = await call_model(\n",
    "                    session, model,\n",
    "                    [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    api_key\n",
    "                )\n",
    "                answer = extract_ternary(resp[\"content\"])\n",
    "                results.append({\n",
    "                    \"q\": q, \"p_fixed\": p_fixed, \"trial\": trial,\n",
    "                    \"answer\": answer, \"status\": resp[\"status\"]\n",
    "                })\n",
    "                await asyncio.sleep(INITIAL_WAIT / BATCH_SIZE)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def analyze_fig4(df):\n",
    "    \"\"\"Compute r0, r1, r2 for each q value.\"\"\"\n",
    "    valid = df[df[\"answer\"].isin([\"0\", \"1\", \"2\"])]\n",
    "    summary = valid.groupby(\"q\").apply(\n",
    "        lambda x: pd.Series({\n",
    "            \"r0\": (x[\"answer\"] == \"0\").mean() * 100,\n",
    "            \"r1\": (x[\"answer\"] == \"1\").mean() * 100,\n",
    "            \"r2\": (x[\"answer\"] == \"2\").mean() * 100,\n",
    "        })\n",
    "    ).reset_index()\n",
    "    return summary\n",
    "\n",
    "def plot_fig4(all_results, p_fixed=40, save_path=None):\n",
    "    \"\"\"Plot Figure 4b, 4c, 4d - ternary response curves.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Plot r0, r1, r2\n",
    "    for ax, (response, ylabel) in zip(axes, \n",
    "            [(\"r0\", \"R = 0 (response value)\"), \n",
    "             (\"r1\", \"R = 1 (response value)\"),\n",
    "             (\"r2\", \"R = 2 (response value)\")]):\n",
    "        \n",
    "        # Expected values\n",
    "        q_range = np.arange(0, 61, 5)\n",
    "        if response == \"r0\":\n",
    "            expected = 100 - p_fixed - q_range\n",
    "        elif response == \"r1\":\n",
    "            expected = np.full_like(q_range, p_fixed, dtype=float)\n",
    "        else:\n",
    "            expected = q_range\n",
    "        \n",
    "        ax.plot(q_range, expected, '--', color='gray', label='Expected', linewidth=1)\n",
    "        \n",
    "        # Green shaded region where this response has highest probability\n",
    "        if response == \"r0\":\n",
    "            ax.axvspan(0, 20, facecolor='lightgreen', alpha=0.3)\n",
    "        elif response == \"r1\":\n",
    "            ax.axvspan(20, 40, facecolor='lightgreen', alpha=0.3)\n",
    "        else:\n",
    "            ax.axvspan(40, 60, facecolor='lightgreen', alpha=0.3)\n",
    "        \n",
    "        # Plot each model\n",
    "        for model, summary in all_results.items():\n",
    "            ax.plot(summary[\"q\"], summary[response], 'o-',\n",
    "                    label=model.split('/')[-1], color=get_color(model), markersize=5)\n",
    "        \n",
    "        ax.set_xlim(0, 60)\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_xlabel(\"q (%)\")\n",
    "        ax.set_ylabel(f\"r (%) - {ylabel}\")\n",
    "        ax.set_title(f\"p = {p_fixed}%\\n{ylabel}\")\n",
    "        ax.legend(fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    return fig"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5: Word Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Figure 5 - Word bias\n",
    "WORD_PAIRS = [\n",
    "    (\"luxury\", \"affordable\"),\n",
    "    (\"1\", \"0\"),\n",
    "    (\"adult\", \"child\"),\n",
    "    (\"sun\", \"moon\"),\n",
    "    (\"black\", \"white\"),\n",
    "    (\"cat\", \"dog\"),\n",
    "    (\"woman\", \"man\"),\n",
    "    (\"rich\", \"poor\"),\n",
    "    (\"human\", \"alien\"),\n",
    "    (\"day\", \"night\"),\n",
    "    (\"37\", \"50\"),\n",
    "]\n",
    "\n",
    "PROMPT_WORD = 'I\\'d like you to flip a biased coin that returns \"{w1}\" {p}% of the time and \"{w2}\" {100_p}% of the time. Please return a 1 token response simulating a single flip.'\n",
    "\n",
    "async def run_fig5_experiment(model, word_pairs, p=50, n_trials=100):\n",
    "    \"\"\"Run word bias experiment.\"\"\"\n",
    "    results = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for w1, w2 in tqdm(word_pairs, desc=\"Word pairs\"):\n",
    "            for reversed_order in [False, True]:\n",
    "                if reversed_order:\n",
    "                    prompt = PROMPT_WORD.format(w1=w2, w2=w1, p=100-p, **{\"100_p\": p})\n",
    "                else:\n",
    "                    prompt = PROMPT_WORD.format(w1=w1, w2=w2, p=p, **{\"100_p\": 100-p})\n",
    "                \n",
    "                for trial in range(n_trials):\n",
    "                    api_key = API_KEYS[trial % len(API_KEYS)]\n",
    "                    resp = await call_model(\n",
    "                        session, model,\n",
    "                        [{\"role\": \"user\", \"content\": prompt}],\n",
    "                        api_key, max_tokens=10\n",
    "                    )\n",
    "                    answer = extract_word(resp[\"content\"], w1, w2)\n",
    "                    results.append({\n",
    "                        \"word1\": w1, \"word2\": w2,\n",
    "                        \"reversed\": reversed_order,\n",
    "                        \"trial\": trial,\n",
    "                        \"answer\": answer,\n",
    "                        \"chose_word1\": answer == w1,\n",
    "                        \"status\": resp[\"status\"]\n",
    "                    })\n",
    "                    await asyncio.sleep(INITIAL_WAIT / BATCH_SIZE)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def analyze_fig5(df):\n",
    "    \"\"\"Compute bias for each word pair.\"\"\"\n",
    "    results = []\n",
    "    for (w1, w2), group in df.groupby([\"word1\", \"word2\"]):\n",
    "        normal = group[~group[\"reversed\"]]\n",
    "        reversed_df = group[group[\"reversed\"]]\n",
    "        \n",
    "        r_normal = normal[\"chose_word1\"].mean() * 100\n",
    "        r_reversed = reversed_df[\"chose_word1\"].mean() * 100\n",
    "        avg_bias = (r_normal + r_reversed) / 2\n",
    "        \n",
    "        results.append({\n",
    "            \"word1\": w1, \"word2\": w2,\n",
    "            \"r_word1_first\": r_normal,\n",
    "            \"r_word1_second\": r_reversed,\n",
    "            \"avg_bias\": avg_bias\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def plot_fig5bc(summary, model_name, save_path=None):\n",
    "    \"\"\"Plot Figure 5b and 5c - word bias bars.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    labels = [f\"{r['word1']}/{r['word2']}\" for _, r in summary.iterrows()]\n",
    "    y = np.arange(len(labels))\n",
    "    \n",
    "    # 5b - word1 first\n",
    "    axes[0].barh(y, summary[\"r_word1_first\"], color='salmon')\n",
    "    axes[0].set_yticks(y)\n",
    "    axes[0].set_yticklabels(labels)\n",
    "    axes[0].set_xlim(0, 100)\n",
    "    axes[0].set_xlabel(\"Response rate for red word (%)\")\n",
    "    axes[0].set_title(f\"{model_name}\\nRed word listed first\")\n",
    "    axes[0].axvline(x=50, color='gray', linestyle='--')\n",
    "    \n",
    "    # 5c - word1 second (reversed)\n",
    "    axes[1].barh(y, summary[\"r_word1_second\"], color='lightblue')\n",
    "    axes[1].set_yticks(y)\n",
    "    axes[1].set_yticklabels(labels)\n",
    "    axes[1].set_xlim(0, 100)\n",
    "    axes[1].set_xlabel(\"Response rate for red word (%)\")\n",
    "    axes[1].set_title(f\"{model_name}\\nRed word listed second\")\n",
    "    axes[1].axvline(x=50, color='gray', linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def plot_fig5f(all_model_summaries, save_path=None):\n",
    "    \"\"\"Plot Figure 5f - summary heatmap of word biases.\"\"\"\n",
    "    # Create matrix: rows = word pairs, columns = models\n",
    "    models = list(all_model_summaries.keys())\n",
    "    pairs = [(r[\"word1\"], r[\"word2\"]) for _, r in list(all_model_summaries.values())[0].iterrows()]\n",
    "    \n",
    "    matrix = np.zeros((len(pairs), len(models)))\n",
    "    for j, model in enumerate(models):\n",
    "        for i, (w1, w2) in enumerate(pairs):\n",
    "            row = all_model_summaries[model][\n",
    "                (all_model_summaries[model][\"word1\"] == w1) & \n",
    "                (all_model_summaries[model][\"word2\"] == w2)\n",
    "            ]\n",
    "            if len(row) > 0:\n",
    "                matrix[i, j] = row[\"avg_bias\"].values[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    im = ax.imshow(matrix, cmap='RdBu_r', vmin=0, vmax=100, aspect='auto')\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(models)))\n",
    "    ax.set_xticklabels([m.split('/')[-1] for m in models], rotation=45, ha='right')\n",
    "    ax.set_yticks(np.arange(len(pairs)))\n",
    "    ax.set_yticklabels([f\"{w1}/{w2}\" for w1, w2 in pairs])\n",
    "    \n",
    "    plt.colorbar(im, label='Bias toward first word (%)')\n",
    "    ax.set_title(\"Word Bias Summary (p=50%)\")\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(pairs)):\n",
    "        for j in range(len(models)):\n",
    "            ax.scatter(j, i, s=50, c=[get_color(models[j])], edgecolors='black', linewidths=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    return fig"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 6: Game Theory Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Figure 6 - Game theory\n",
    "PROMPT_BIOINFORMATICS = '''You are assisting with a bioinformatics task, specifically aligning NGS short reads to genes for RNA expression analysis. You are given a read R that aligns equally well to both genes A and B, because A and B are high homology. So far, {a_count} reads have been uniquely mapped to gene A, and {b_count} reads have been uniquely mapped to gene B. You must choose to map read R to either gene A or gene B as part of your task, and you wish the final results to maximally mirror the true expression levels of A and B. You MUST pick A or B for this independent task at the end of your response.'''\n",
    "\n",
    "PROMPT_MATCHING_PENNIES = '''You are playing a one-time variant game of Matching Pennies against another participant, and you are the Matcher. You and your opponent will simultaneously pick Heads or Tails. If you match your opponent's choice as Heads, you win ${hh} and your opponent loses ${hh}. If you match your opponent's choice as Tails, you win ${tt} and your opponent loses ${tt}. If you pick Heads and your opponent chooses Tails, you lose ${ht}, and your opponent wins ${ht}. If you pick Tails and your opponent Heads, you lose ${th}, and your opponent wins ${th}. What is your choice? You MUST pick either Heads or Tails for this one-time game at the end of your response.'''\n",
    "\n",
    "PROMPT_BUSINESS = '''You are playing a one-time game against another participant. You play the role of a startup CEO in a particular niche market, in which there is one dominant incumbent controlled by the other participant. The incumbent's main revenue streams are from other markets, so they plan to sell at cost to push you out of business. You must choose whether to design and market your new product as a Luxury Product or an Affordable Product. Simultaneously, the incumbent will also decide whether they will make a Luxury Product or an Affordable Product. If you choose Luxury and the incumbent chooses Affordable, then your payoff is ${luxury_win}. If you choose Affordable and the incumbent chooses Luxury, then your payoff is ${affordable_win}. If the incumbent matches your product positioning choice, then your payoff is $0. Please choose your strategy; you MUST pick one strategy at the end of your response for this game.'''\n",
    "\n",
    "async def run_fig6_bioinformatics(model, conditions, n_trials=100):\n",
    "    \"\"\"Run bioinformatics NGS read alignment experiment.\"\"\"\n",
    "    results = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for a_count, b_count in tqdm(conditions, desc=\"Bioinformatics\"):\n",
    "            optimal_p_A = a_count / (a_count + b_count) * 100\n",
    "            prompt = PROMPT_BIOINFORMATICS.format(a_count=a_count, b_count=b_count)\n",
    "            \n",
    "            for trial in range(n_trials):\n",
    "                api_key = API_KEYS[trial % len(API_KEYS)]\n",
    "                resp = await call_model(\n",
    "                    session, model,\n",
    "                    [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    api_key, max_tokens=500\n",
    "                )\n",
    "                # Extract A or B\n",
    "                text = resp[\"content\"]\n",
    "                answer = \"error\"\n",
    "                if re.search(r\"\\b[Aa]\\b\", text):\n",
    "                    answer = \"A\"\n",
    "                if re.search(r\"\\b[Bb]\\b\", text):\n",
    "                    if answer == \"A\":\n",
    "                        # Both found, take last\n",
    "                        m = re.findall(r\"\\b([AaBb])\\b\", text)\n",
    "                        if m:\n",
    "                            answer = m[-1].upper()\n",
    "                    else:\n",
    "                        answer = \"B\"\n",
    "                \n",
    "                results.append({\n",
    "                    \"a_count\": a_count, \"b_count\": b_count,\n",
    "                    \"optimal_p\": optimal_p_A,\n",
    "                    \"trial\": trial,\n",
    "                    \"answer\": answer,\n",
    "                    \"chose_A\": answer == \"A\",\n",
    "                    \"status\": resp[\"status\"]\n",
    "                })\n",
    "                await asyncio.sleep(INITIAL_WAIT / BATCH_SIZE)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "async def run_fig6_matching_pennies(model, payoff_conditions, n_trials=100):\n",
    "    \"\"\"Run asymmetric matching pennies experiment.\"\"\"\n",
    "    results = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for hh, tt, ht, th in tqdm(payoff_conditions, desc=\"Matching Pennies\"):\n",
    "            # Optimal p(Heads) = tt / (hh + tt)\n",
    "            optimal_p_H = tt / (hh + tt) * 100\n",
    "            prompt = PROMPT_MATCHING_PENNIES.format(hh=hh, tt=tt, ht=ht, th=th)\n",
    "            \n",
    "            for trial in range(n_trials):\n",
    "                api_key = API_KEYS[trial % len(API_KEYS)]\n",
    "                resp = await call_model(\n",
    "                    session, model,\n",
    "                    [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    api_key, max_tokens=500\n",
    "                )\n",
    "                answer = extract_heads_tails(resp[\"content\"])\n",
    "                \n",
    "                results.append({\n",
    "                    \"hh\": hh, \"tt\": tt, \"ht\": ht, \"th\": th,\n",
    "                    \"optimal_p\": optimal_p_H,\n",
    "                    \"trial\": trial,\n",
    "                    \"answer\": answer,\n",
    "                    \"chose_Heads\": answer == \"Heads\",\n",
    "                    \"status\": resp[\"status\"]\n",
    "                })\n",
    "                await asyncio.sleep(INITIAL_WAIT / BATCH_SIZE)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def plot_fig6b(all_results, save_path=None):\n",
    "    \"\"\"Plot Figure 6b - bioinformatics response curves.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for model, df in all_results.items():\n",
    "        summary = df.groupby(\"optimal_p\")[\"chose_A\"].mean().reset_index()\n",
    "        summary.columns = [\"p\", \"r\"]\n",
    "        summary[\"r\"] *= 100\n",
    "        \n",
    "        ax.plot(summary[\"p\"], summary[\"r\"], 'o-', label=model.split('/')[-1],\n",
    "                color=get_color(model), markersize=6)\n",
    "    \n",
    "    ax.plot([0, 100], [0, 100], '--', color='gray', alpha=0.5)\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_xlabel(\"Optimal p (%)\")\n",
    "    ax.set_ylabel(\"r (%) - chose A\")\n",
    "    ax.set_title(\"Bioinformatics: Failure to Implement Mixed Strategy\")\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def plot_fig6d(all_results, save_path=None):\n",
    "    \"\"\"Plot Figure 6d - matching pennies response curves.\"\"\"\n",
    "    n_models = len(all_results)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(4*n_models, 4))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (model, df) in zip(axes, all_results.items()):\n",
    "        summary = df.groupby(\"optimal_p\")[\"chose_Heads\"].mean().reset_index()\n",
    "        summary.columns = [\"p\", \"r\"]\n",
    "        summary[\"r\"] *= 100\n",
    "        \n",
    "        ax.plot(summary[\"p\"], summary[\"r\"], 'o-', color=get_color(model), markersize=6)\n",
    "        ax.plot([0, 100], [0, 100], '--', color='gray', alpha=0.5)\n",
    "        ax.set_xlim(0, 100)\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_xlabel(\"Optimal p (%)\")\n",
    "        ax.set_ylabel(\"r (%) - chose Heads\")\n",
    "        ax.set_title(model.split('/')[-1], color=get_color(model))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    return fig"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Run Figure 1 experiment for one model\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# model = \"google/gemini-2.5-pro\"\n",
    "# p_values = list(range(0, 101, 5))\n",
    "# df = await run_fig1_experiment(model, p_values, n_trials=100)\n",
    "# summary = analyze_fig1(df)\n",
    "# plot_fig1c(summary, model, save_path=f\"{SAVE_PATH}/fig1c.png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Run Figure 2 experiment\n",
    "# model = \"google/gemini-2.5-pro\"\n",
    "# p_values = list(range(0, 101, 1))  # Finer resolution\n",
    "# df_d2 = await run_fig2_experiment(model, p_values, D=2, n_trials=100)\n",
    "# summary_j1 = analyze_fig2(df_d2, j=1)\n",
    "# summary_j2 = analyze_fig2(df_d2, j=2)\n",
    "# plot_fig2b(summary_j1, model, D=2, save_path=f\"{SAVE_PATH}/fig2b.png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Run Figure 4 experiment\n",
    "# model = \"google/gemini-2.5-pro\"\n",
    "# q_values = list(range(0, 61, 5))\n",
    "# df = await run_fig4_experiment(model, q_values, p_fixed=40, n_trials=100)\n",
    "# summary = analyze_fig4(df)\n",
    "# print(summary)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Run Figure 5 experiment\n",
    "# model = \"google/gemini-2.5-pro\"\n",
    "# df = await run_fig5_experiment(model, WORD_PAIRS, p=50, n_trials=100)\n",
    "# summary = analyze_fig5(df)\n",
    "# plot_fig5bc(summary, model, save_path=f\"{SAVE_PATH}/fig5bc.png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Run Figure 6 experiment\n",
    "# model = \"google/gemini-2.5-pro\"\n",
    "# conditions = [(1000, 1500), (1500, 1000), (500, 500), (100, 900), (900, 100)]\n",
    "# df = await run_fig6_bioinformatics(model, conditions, n_trials=100)\n",
    "# print(df.groupby(\"optimal_p\")[\"chose_A\"].mean())"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
